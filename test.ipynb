{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GETTING STARTED WITH RAG APPLICATIONS\n",
    "1. LOAD THE SOURCE FILE -> PDFs, Text files\n",
    "2. TRANSFORMATION OF THE LOADED DATA\n",
    "- BREAKING INTO CHUNKS\n",
    "3. CREATE EMBEDDINGS FOR THE CHUNKS OF DOCUMENTS -> STORE THEM INTO VECTOR DB -> QUERY USING SIMILARITY SEARCH FROM CHROMADB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading text files\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('test.txt')\n",
    "result = loader.load()\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pdf files\n",
    "# install pypdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('abstract.pdf')\n",
    "result = loader.load()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation\n",
    "# breaking the loaded data into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "splitted_docs = text_splitter.split_documents(result) # used for splitting the document into small chunks of document\n",
    "# print(splitted_docs[:5])\n",
    "for item in splitted_docs[:5]:\n",
    "    print(item.page_content)\n",
    "    print(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embeddings for the document chunks\n",
    "from langchain_ollama import OllamaEmbeddings # using ollama embeddings\n",
    "embeddings_object = OllamaEmbeddings(model=\"phi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTOR DB\n",
    "\n",
    "Vector databases are specialized tools that help store and quickly search through data represented as numerical codes, known as vectors. These vectors capture important characteristics of data like text, images, or sounds. The main purpose of vector databases is to find similar items efficiently. For example, if you want to find images that look like a given picture or texts that have a similar meaning, vector databases make this process fast and effective, even when dealing with a large amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing embeddings into vector db\n",
    "# install chromadb\n",
    "from langchain_community.vectorstores import Chroma # using chroma vector db inbuilt in langchainn\n",
    "db = Chroma.from_documents(splitted_docs[:5], embeddings_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the vector db for similar embeddings\n",
    "query = \"who are the team members\"\n",
    "res = db.similarity_search(query) # note this does not use llm\n",
    "for item in res:\n",
    "    print(item.page_content)\n",
    "    print(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens behind the scenes is that\n",
    "when a query is sent to the llm to query the vector db, the query is first transformed into embeddings using the same technique/algo used to transform the document, after which the most similar embeddings are returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET US USE LLMS TO QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading open source llm\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"phi\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design the chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question only based on the provided context only\n",
    "<context>{context}</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing chains\n",
    "# create stuff document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating retrievers (interface) that is connected to vector store\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating retriever chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retriever_chain.invoke({\"input\": \"what is the content of the pdf\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1_env",
   "language": "python",
   "name": "project1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
